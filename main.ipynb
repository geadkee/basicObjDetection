{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection through images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pillow scipy matplotlib==3.3.2 opencv-python keras-resnet==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cython opencv-python>=4.1.2 torch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cu102 torchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cu102 pytest==7.1.3 tqdm==4.64.1 mock==4.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imageai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gead Kee\\.conda\\envs\\imageClassification-tensorF\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gead Kee\\.conda\\envs\\imageClassification-tensorF\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Gead Kee\\.conda\\envs\\imageClassification-tensorF\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gead Kee\\.conda\\envs\\imageClassification-tensorF\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus  :  97.7\n",
      "person  :  93.65\n",
      "car  :  91.31\n",
      "car  :  82.49\n",
      "person  :  77.17\n",
      "car  :  74.42\n",
      "car  :  74.04\n",
      "car  :  70.32\n",
      "truck  :  70.25\n",
      "truck  :  62.22\n",
      "person  :  59.26\n",
      "car  :  58.07\n",
      "car  :  55.01\n",
      "car  :  54.65\n",
      "person  :  54.38\n",
      "car  :  52.47\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "# detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\"))\n",
    "detector.setModelPath( os.path.join(execution_path , \"retinanet_resnet50_fpn_coco-eeacb38b.pth\"))\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"image2.jpg\"), output_image_path=os.path.join(execution_path , \"image2new.jpg\"))\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person  :  99.99  :  [991, 608, 1110, 916]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [69, 612, 216, 1000]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [832, 619, 917, 875]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [544, 620, 635, 965]\n",
      "--------------------------------\n",
      "person  :  99.83  :  [613, 607, 714, 938]\n",
      "--------------------------------\n",
      "person  :  99.98  :  [209, 639, 308, 952]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [912, 587, 1000, 862]\n",
      "--------------------------------\n",
      "person  :  99.98  :  [402, 627, 494, 910]\n",
      "--------------------------------\n",
      "person  :  99.99  :  [343, 623, 418, 869]\n",
      "--------------------------------\n",
      "person  :  99.9  :  [4, 619, 77, 930]\n",
      "--------------------------------\n",
      "person  :  99.87  :  [114, 586, 1156, 848]\n",
      "--------------------------------\n",
      "person  :  99.79  :  [1244, 560, 1431, 977]\n",
      "--------------------------------\n",
      "person  :  99.96  :  [725, 605, 801, 847]\n",
      "--------------------------------\n",
      "person  :  99.95  :  [286, 627, 354, 879]\n",
      "--------------------------------\n",
      "person  :  99.97  :  [1073, 598, 1143, 890]\n",
      "--------------------------------\n",
      "person  :  99.88  :  [1153, 599, 1239, 881]\n",
      "--------------------------------\n",
      "person  :  99.26  :  [1435, 661, 1499, 963]\n",
      "--------------------------------\n",
      "person  :  98.58  :  [446, 720, 526, 934]\n",
      "--------------------------------\n",
      "person  :  98.45  :  [511, 637, 586, 918]\n",
      "--------------------------------\n",
      "handbag  :  97.92  :  [1291, 721, 1412, 929]\n",
      "--------------------------------\n",
      "handbag  :  93.24  :  [309, 665, 357, 772]\n",
      "--------------------------------\n",
      "handbag  :  91.01  :  [626, 654, 691, 786]\n",
      "--------------------------------\n",
      "suitcase  :  93.69  :  [773, 758, 827, 841]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"yolov3.pt\"))\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"image1.jpg\"), output_image_path=os.path.join(execution_path , \"image1new.jpg\"), minimum_percentage_probability=30)\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageClassification-tensorF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "342ff15cb6421c59df2ab9cf3e7f208af842365a113f7bbacc8f6bdbb97b2269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
